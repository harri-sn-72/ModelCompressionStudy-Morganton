{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compression Comparative Study\n",
    "\n",
    "**Hardware Platforms:**\n",
    "- Google Colab GPU (Cloud)\n",
    "- HP Omnibook Intel Arc GPU (Local)\n",
    "- Lenovo ThinkCentre (Intel Core i5 Desktop)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB ONLY:\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install matplotlib pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block below imports PyTorch and its common submodules along with supporting libraries, sets fixed random seeds for reproducibility, and prints the PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.10.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This helper function checks for an Intel XPU first, then a CUDA GPU, and falls back to CPU and prints which device it selects. Important since this code is utilized on multiple platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU (no GPU detected)\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    \"\"\"\n",
    "    Auto-detect the best available device for training.\n",
    "    Priority: Intel XPU > CUDA > CPU\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import intel_extension_for_pytorch as ipex\n",
    "        if torch.xpu.is_available():\n",
    "            print(\"Using Intel XPU (Arc GPU)\")\n",
    "            return torch.device('xpu')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        return torch.device('cuda')\n",
    "    \n",
    "    print(\"Using CPU (no GPU detected)\")\n",
    "    return torch.device('cpu')\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section sets batch parameters, defines normalization transforms for MNIST and CIFAR‑10, downloads both datasets, wraps them in `DataLoader`'s, and prints the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Loading CIFAR-10 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harri\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ MNIST: 60000 train, 10000 test\n",
      "✓ CIFAR-10: 50000 train, 10000 test\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128 \n",
    "NUM_WORKERS = 2 \n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  \n",
    "])\n",
    "\n",
    "cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)) \n",
    "])\n",
    "\n",
    "print(\"Loading MNIST dataset...\")\n",
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=mnist_transform\n",
    ")\n",
    "mnist_test = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=mnist_transform\n",
    ")\n",
    "\n",
    "print(\"Loading CIFAR-10 dataset...\")\n",
    "cifar_train = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=cifar_transform\n",
    ")\n",
    "cifar_test = torchvision.datasets.CIFAR10(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=cifar_transform\n",
    ")\n",
    "\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "cifar_train_loader = DataLoader(cifar_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "cifar_test_loader = DataLoader(cifar_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"\\n✓ MNIST: {len(mnist_train)} train, {len(mnist_test)} test\")\n",
    "print(f\"✓ CIFAR-10: {len(cifar_train)} train, {len(cifar_test)} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is where 2 simple convolutional neural networks are defined: `SimpleMNIST` for grayscale 28×28 digits and `SimpleCIFAR` for color 32×32 images, both with a few conv/pool layers followed by fully‑connected layers and dropout, and then instances are created and moved to the chosen device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating models.\n",
      "MNIST model created with 421,642 parameters\n",
      "CIFAR model created with 2,473,610 parameters\n"
     ]
    }
   ],
   "source": [
    "class SimpleMNIST(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for MNIST (grayscale 28x28 images)\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv1: 1 -> 32 channels, 3x3 kernel\n",
    "    - Conv2: 32 -> 64 channels, 3x3 kernel\n",
    "    - FC1: 9216 -> 128 neurons\n",
    "    - FC2: 128 -> 10 classes (digits 0-9)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleMNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = x.view(-1, 64 * 7 * 7)          \n",
    "        x = F.relu(self.fc1(x))              \n",
    "        x = self.dropout(x)                  \n",
    "        x = self.fc2(x)                     \n",
    "        return x\n",
    "\n",
    "\n",
    "class SimpleCIFAR(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for CIFAR-10 (color 32x32 images)\n",
    "    \n",
    "    Architecture:\n",
    "    - Conv1: 3 -> 64 channels, 3x3 kernel\n",
    "    - Conv2: 64 -> 128 channels, 3x3 kernel\n",
    "    - Conv3: 128 -> 256 channels, 3x3 kernel\n",
    "    - FC1: 4096 -> 512 neurons\n",
    "    - FC2: 512 -> 10 classes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SimpleCIFAR, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  \n",
    "        x = self.pool(F.relu(self.conv2(x)))  \n",
    "        x = self.pool(F.relu(self.conv3(x)))  \n",
    "        x = x.view(-1, 256 * 4 * 4)           \n",
    "        x = F.relu(self.fc1(x))             \n",
    "        x = self.dropout(x)                  \n",
    "        x = self.fc2(x)                    \n",
    "        return x\n",
    "\n",
    "print(\"Creating models.\")\n",
    "mnist_model = SimpleMNIST().to(device)\n",
    "cifar_model = SimpleCIFAR().to(device)\n",
    "\n",
    "print(f\"MNIST model created with {sum(p.numel() for p in mnist_model.parameters()):,} parameters\")\n",
    "print(f\"CIFAR model created with {sum(p.numel() for p in cifar_model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train_model` function loops over epochs and batches, computes loss/accuracy, performs backpropagation and optimizer steps, and returns the epoch-wise loss history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    \"\"\"\n",
    "    Train a neural network model.\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network to train\n",
    "        train_loader: DataLoader with training data\n",
    "        criterion: Loss function (how wrong predictions are)\n",
    "        optimizer: Algorithm to update weights\n",
    "        epochs: Number of times to see the entire dataset\n",
    "    \n",
    "    Returns:\n",
    "        List of losses per epoch\n",
    "    \"\"\"\n",
    "    model.train() \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        losses.append(epoch_loss)\n",
    "        print(f'\\n>>> Epoch {epoch+1} complete. Avg Loss: {epoch_loss:.4f}, '\n",
    "              f'Train Accuracy: {100.*correct/total:.2f}%\\n')\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"\n",
    "    Evaluate model on test data.\n",
    "    \n",
    "    Returns:\n",
    "        accuracy: Percentage of correct predictions (0-100)\n",
    "    \"\"\"\n",
    "    model.eval()  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"\n",
    "    Calculate model size in megabytes (MB).\n",
    "    \n",
    "    Each parameter is a 32-bit float = 4 bytes\n",
    "    \"\"\"\n",
    "    param_size = sum(p.numel() for p in model.parameters()) * 4  \n",
    "    buffer_size = sum(b.numel() for b in model.buffers()) * 4\n",
    "    size_mb = (param_size + buffer_size) / 1024**2\n",
    "    return size_mb\n",
    "\n",
    "\n",
    "def measure_inference_time(model, test_loader, num_iterations=100):\n",
    "    \"\"\"\n",
    "    Measure average inference time per image (in milliseconds).\n",
    "    \n",
    "    Args:\n",
    "        model: The neural network\n",
    "        test_loader: DataLoader with test data\n",
    "        num_iterations: Number of batches to average over\n",
    "    \n",
    "    Returns:\n",
    "        avg_time_ms: Average time per single image in milliseconds\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            if i >= num_iterations:\n",
    "                break\n",
    "            \n",
    "            data = data.to(device)\n",
    "            \n",
    "            if i == 0:\n",
    "                _ = model(data)\n",
    "                continue\n",
    "            \n",
    "            start = time.time()\n",
    "            _ = model(data)\n",
    "            \n",
    "            if device.type == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            elif device.type == 'xpu':\n",
    "                torch.xpu.synchronize()\n",
    "            \n",
    "            end = time.time()\n",
    "            \n",
    "            batch_time = (end - start) * 1000 \n",
    "            per_image_time = batch_time / data.size(0)\n",
    "            times.append(per_image_time)\n",
    "    \n",
    "    avg_time_ms = np.mean(times)\n",
    "    return avg_time_ms\n",
    "\n",
    "\n",
    "def collect_metrics(model, test_loader, model_name):\n",
    "    \"\"\"\n",
    "    Collect all metrics for a model.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with accuracy, size, and latency\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    \n",
    "    accuracy = evaluate_model(model, test_loader)\n",
    "    size_mb = get_model_size(model)\n",
    "    latency_ms = measure_inference_time(model, test_loader)\n",
    "    \n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'accuracy': accuracy,\n",
    "        'size_mb': size_mb,\n",
    "        'latency_ms': latency_ms,\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"  Size: {size_mb:.2f} MB\")\n",
    "    print(f\"  Latency: {latency_ms:.4f} ms/image\")\n",
    "    print()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the script sets the learning rate and number of epochs, initializes a results list, and prints a header announcing the start of baseline model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING BASELINE MODELS\n",
      "------------------------------------------------------------\n",
      "\n",
      "[1/2] Training MNIST Baseline...\n",
      "\n",
      "Epoch 1/10, Batch 0/469, Loss: 2.3213, Acc: 7.03%\n",
      "Epoch 1/10, Batch 100/469, Loss: 0.3103, Acc: 79.46%\n",
      "Epoch 1/10, Batch 200/469, Loss: 0.1830, Acc: 86.66%\n",
      "Epoch 1/10, Batch 300/469, Loss: 0.1263, Acc: 89.71%\n",
      "Epoch 1/10, Batch 400/469, Loss: 0.2008, Acc: 91.40%\n",
      "\n",
      ">>> Epoch 1 complete. Avg Loss: 0.2543, Train Accuracy: 92.16%\n",
      "\n",
      "Epoch 2/10, Batch 0/469, Loss: 0.1128, Acc: 96.88%\n",
      "Epoch 2/10, Batch 100/469, Loss: 0.0707, Acc: 97.17%\n",
      "Epoch 2/10, Batch 200/469, Loss: 0.0616, Acc: 97.31%\n",
      "Epoch 2/10, Batch 300/469, Loss: 0.0709, Acc: 97.31%\n",
      "Epoch 2/10, Batch 400/469, Loss: 0.0472, Acc: 97.47%\n",
      "\n",
      ">>> Epoch 2 complete. Avg Loss: 0.0870, Train Accuracy: 97.47%\n",
      "\n",
      "Epoch 3/10, Batch 0/469, Loss: 0.0381, Acc: 99.22%\n",
      "Epoch 3/10, Batch 100/469, Loss: 0.0819, Acc: 98.14%\n",
      "Epoch 3/10, Batch 200/469, Loss: 0.0586, Acc: 97.91%\n",
      "Epoch 3/10, Batch 300/469, Loss: 0.0723, Acc: 98.05%\n",
      "Epoch 3/10, Batch 400/469, Loss: 0.0945, Acc: 98.07%\n",
      "\n",
      ">>> Epoch 3 complete. Avg Loss: 0.0654, Train Accuracy: 98.10%\n",
      "\n",
      "Epoch 4/10, Batch 0/469, Loss: 0.0561, Acc: 97.66%\n",
      "Epoch 4/10, Batch 100/469, Loss: 0.1123, Acc: 98.50%\n",
      "Epoch 4/10, Batch 200/469, Loss: 0.0438, Acc: 98.46%\n",
      "Epoch 4/10, Batch 300/469, Loss: 0.0634, Acc: 98.47%\n",
      "Epoch 4/10, Batch 400/469, Loss: 0.0231, Acc: 98.47%\n",
      "\n",
      ">>> Epoch 4 complete. Avg Loss: 0.0527, Train Accuracy: 98.48%\n",
      "\n",
      "Epoch 5/10, Batch 0/469, Loss: 0.0069, Acc: 100.00%\n",
      "Epoch 5/10, Batch 100/469, Loss: 0.0134, Acc: 98.58%\n",
      "Epoch 5/10, Batch 200/469, Loss: 0.0781, Acc: 98.61%\n",
      "Epoch 5/10, Batch 300/469, Loss: 0.1171, Acc: 98.66%\n",
      "Epoch 5/10, Batch 400/469, Loss: 0.0376, Acc: 98.65%\n",
      "\n",
      ">>> Epoch 5 complete. Avg Loss: 0.0446, Train Accuracy: 98.67%\n",
      "\n",
      "Epoch 6/10, Batch 0/469, Loss: 0.0402, Acc: 97.66%\n",
      "Epoch 6/10, Batch 100/469, Loss: 0.1058, Acc: 98.89%\n",
      "Epoch 6/10, Batch 200/469, Loss: 0.0165, Acc: 98.85%\n",
      "Epoch 6/10, Batch 300/469, Loss: 0.0590, Acc: 98.82%\n",
      "Epoch 6/10, Batch 400/469, Loss: 0.0094, Acc: 98.82%\n",
      "\n",
      ">>> Epoch 6 complete. Avg Loss: 0.0393, Train Accuracy: 98.79%\n",
      "\n",
      "Epoch 7/10, Batch 0/469, Loss: 0.0182, Acc: 100.00%\n",
      "Epoch 7/10, Batch 100/469, Loss: 0.0340, Acc: 99.09%\n",
      "Epoch 7/10, Batch 200/469, Loss: 0.0069, Acc: 99.01%\n",
      "Epoch 7/10, Batch 300/469, Loss: 0.0749, Acc: 98.99%\n",
      "Epoch 7/10, Batch 400/469, Loss: 0.0155, Acc: 98.99%\n",
      "\n",
      ">>> Epoch 7 complete. Avg Loss: 0.0341, Train Accuracy: 98.99%\n",
      "\n",
      "Epoch 8/10, Batch 0/469, Loss: 0.0078, Acc: 100.00%\n",
      "Epoch 8/10, Batch 100/469, Loss: 0.0225, Acc: 99.22%\n",
      "Epoch 8/10, Batch 200/469, Loss: 0.0123, Acc: 99.13%\n",
      "Epoch 8/10, Batch 300/469, Loss: 0.0260, Acc: 99.06%\n",
      "Epoch 8/10, Batch 400/469, Loss: 0.0674, Acc: 99.07%\n",
      "\n",
      ">>> Epoch 8 complete. Avg Loss: 0.0298, Train Accuracy: 99.08%\n",
      "\n",
      "Epoch 9/10, Batch 0/469, Loss: 0.0153, Acc: 99.22%\n",
      "Epoch 9/10, Batch 100/469, Loss: 0.0040, Acc: 99.16%\n",
      "Epoch 9/10, Batch 200/469, Loss: 0.0299, Acc: 99.16%\n",
      "Epoch 9/10, Batch 300/469, Loss: 0.1272, Acc: 99.11%\n",
      "Epoch 9/10, Batch 400/469, Loss: 0.0394, Acc: 99.13%\n",
      "\n",
      ">>> Epoch 9 complete. Avg Loss: 0.0256, Train Accuracy: 99.15%\n",
      "\n",
      "Epoch 10/10, Batch 0/469, Loss: 0.0298, Acc: 99.22%\n",
      "Epoch 10/10, Batch 100/469, Loss: 0.0091, Acc: 99.20%\n",
      "Epoch 10/10, Batch 200/469, Loss: 0.0018, Acc: 99.21%\n",
      "Epoch 10/10, Batch 300/469, Loss: 0.0504, Acc: 99.19%\n",
      "Epoch 10/10, Batch 400/469, Loss: 0.0235, Acc: 99.20%\n",
      "\n",
      ">>> Epoch 10 complete. Avg Loss: 0.0248, Train Accuracy: 99.20%\n",
      "\n",
      "Evaluating MNIST_Baseline...\n",
      "  Accuracy: 99.34%\n",
      "  Size: 1.61 MB\n",
      "  Latency: 0.1857 ms/image\n",
      "\n",
      "MNIST model saved to mnist_baseline.pth\n",
      "\n",
      "\n",
      "[2/2] Training CIFAR-10 Baseline...\n",
      "\n",
      "Epoch 1/10, Batch 0/391, Loss: 2.2932, Acc: 14.06%\n",
      "Epoch 1/10, Batch 100/391, Loss: 1.5037, Acc: 33.20%\n",
      "Epoch 1/10, Batch 200/391, Loss: 1.2424, Acc: 41.40%\n",
      "Epoch 1/10, Batch 300/391, Loss: 1.2070, Acc: 46.34%\n",
      "\n",
      ">>> Epoch 1 complete. Avg Loss: 1.4004, Train Accuracy: 49.23%\n",
      "\n",
      "Epoch 2/10, Batch 0/391, Loss: 0.9810, Acc: 61.72%\n",
      "Epoch 2/10, Batch 100/391, Loss: 1.1060, Acc: 63.99%\n",
      "Epoch 2/10, Batch 200/391, Loss: 0.8947, Acc: 64.67%\n",
      "Epoch 2/10, Batch 300/391, Loss: 0.8946, Acc: 65.26%\n",
      "\n",
      ">>> Epoch 2 complete. Avg Loss: 0.9599, Train Accuracy: 66.28%\n",
      "\n",
      "Epoch 3/10, Batch 0/391, Loss: 0.7153, Acc: 76.56%\n",
      "Epoch 3/10, Batch 100/391, Loss: 0.7051, Acc: 72.33%\n",
      "Epoch 3/10, Batch 200/391, Loss: 0.6962, Acc: 72.47%\n",
      "Epoch 3/10, Batch 300/391, Loss: 0.7423, Acc: 72.50%\n",
      "\n",
      ">>> Epoch 3 complete. Avg Loss: 0.7748, Train Accuracy: 72.91%\n",
      "\n",
      "Epoch 4/10, Batch 0/391, Loss: 0.7184, Acc: 72.66%\n",
      "Epoch 4/10, Batch 100/391, Loss: 0.5832, Acc: 78.02%\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10  \n",
    "\n",
    "all_results = []\n",
    "\n",
    "print(\"TRAINING BASELINE MODELS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n[1/2] Training MNIST Baseline...\\n\")\n",
    "mnist_model = SimpleMNIST().to(device)\n",
    "mnist_criterion = nn.CrossEntropyLoss()\n",
    "mnist_optimizer = optim.Adam(mnist_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "mnist_losses = train_model(mnist_model, mnist_train_loader, mnist_criterion, mnist_optimizer, epochs=EPOCHS)\n",
    "\n",
    "mnist_metrics = collect_metrics(mnist_model, mnist_test_loader, \"MNIST_Baseline\")\n",
    "all_results.append(mnist_metrics)\n",
    "\n",
    "torch.save(mnist_model.state_dict(), 'mnist_baseline.pth')\n",
    "print(\"MNIST model saved to mnist_baseline.pth\\n\")\n",
    "\n",
    "print(\"\\n[2/2] Training CIFAR-10 Baseline...\\n\")\n",
    "cifar_model = SimpleCIFAR().to(device)\n",
    "cifar_criterion = nn.CrossEntropyLoss()\n",
    "cifar_optimizer = optim.Adam(cifar_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "cifar_losses = train_model(cifar_model, cifar_train_loader, cifar_criterion, cifar_optimizer, epochs=EPOCHS)\n",
    "\n",
    "cifar_metrics = collect_metrics(cifar_model, cifar_test_loader, \"CIFAR_Baseline\")\n",
    "all_results.append(cifar_metrics)\n",
    "\n",
    "torch.save(cifar_model.state_dict(), 'cifar_baseline.pth')\n",
    "print(\"CIFAR model saved to cifar_baseline.pth\\n\")\n",
    "\n",
    "print(\"BASELINE TRAINING COMPLETE!\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(\"\\nBaseline Results:\")\n",
    "print(results_df[['model_name', 'accuracy', 'size_mb', 'latency_ms']])\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv('baseline_results.csv', index=False)\n",
    "print(\"\\nResults saved to baseline_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
