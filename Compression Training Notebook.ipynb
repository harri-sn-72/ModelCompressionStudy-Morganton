{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Compression Techniques - All Compressions\n",
    "\n",
    "This notebook implements three compression techniques:\n",
    "1. **Pruning**: Remove unimportant weights\n",
    "2. **Quantization**: Reduce numerical precision  \n",
    "3. **Knowledge Distillation**: Train smaller model from larger one\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block below imports PyTorch and its common submodules along with supporting libraries, sets fixed random seeds for reproducibility, and prints the PyTorch version from the baseline notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ã¢Å“â€œ Helper functions loaded successfully!\n",
      "Ã¢Å¡Â  Using CPU (this will be slower)\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "exec(open('helper_functions.py').read())\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell below sets up batch size and transforms, loads the MNIST/CIFAR-10 datasets with those transforms, wraps them in `DataLoader`s, then restores the saved baseline models and previous results for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harri\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Datasets loaded\n",
      "âœ“ Helper functions loaded successfully!\n",
      "âœ“ MNIST baseline loaded\n",
      "âœ“ CIFAR baseline loaded\n",
      "âœ“ Loaded 2 baseline results\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "cifar_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n",
    "mnist_test = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=mnist_transform)\n",
    "cifar_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=cifar_transform)\n",
    "cifar_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=cifar_transform)\n",
    "\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "cifar_train_loader = DataLoader(cifar_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "cifar_test_loader = DataLoader(cifar_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Datasets loaded\")\n",
    "\n",
    "from helper_functions import SimpleMNIST, SimpleCIFAR\n",
    "\n",
    "mnist_baseline = SimpleMNIST().to(device)\n",
    "mnist_baseline.load_state_dict(torch.load('mnist_baseline.pth'))\n",
    "print(\"MNIST baseline loaded\")\n",
    "\n",
    "cifar_baseline = SimpleCIFAR().to(device)\n",
    "cifar_baseline.load_state_dict(torch.load('cifar_baseline.pth'))\n",
    "print(\"CIFAR baseline loaded\")\n",
    "\n",
    "all_results = pd.read_csv('baseline_results.csv').to_dict('records')\n",
    "print(f\"Loaded {len(all_results)} baseline results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pruning**\n",
    "\n",
    "These helper routines find all conv/linear layers in a model, prune a specified fraction of the smallestâ€‘magnitude weights globally (and then make that pruning permanent), and compute the resulting percentage of zeroedâ€‘out parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Pruning functions defined\n"
     ]
    }
   ],
   "source": [
    "def apply_unstructured_pruning(model, amount=0.3):\n",
    "    \"\"\"\n",
    "    Apply magnitude-based unstructured pruning to all Conv2d and Linear layers.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network to prune\n",
    "        amount: Fraction of weights to prune (0.3 = 30%)\n",
    "    \n",
    "    Returns:\n",
    "        Pruned model\n",
    "    \"\"\"\n",
    "    parameters_to_prune = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "            parameters_to_prune.append((module, 'weight'))\n",
    "    \n",
    "\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount,\n",
    "    )\n",
    "    \n",
    "    for module, param_name in parameters_to_prune:\n",
    "        prune.remove(module, param_name)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_sparsity(model):\n",
    "    \"\"\"\n",
    "    Calculate the percentage of zero weights in the model.\n",
    "    \n",
    "    Returns:\n",
    "        sparsity: Percentage of weights that are zero (0-100)\n",
    "    \"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        total_params += param.numel()\n",
    "        zero_params += (param == 0).sum().item()\n",
    "    \n",
    "    sparsity = 100. * zero_params / total_params\n",
    "    return sparsity\n",
    "\n",
    "print(\"Pruning functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pruning \n",
    "\n",
    "Test different pruning levels: 30%, 50%, 70%, 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PRUNING EXPERIMENTS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Testing 30% Pruning\n",
      "============================================================\n",
      "\n",
      "[1/2] Pruning MNIST model (30%)...\n",
      "Sparsity: 29.98%\n",
      "Evaluating MNIST_Pruned_30%...\n",
      "  Accuracy: 99.33%\n",
      "  Size: 1.61 MB\n",
      "  Latency: 0.2099 ms/image\n",
      "\n",
      "\n",
      "[2/2] Pruning CIFAR model (30%)...\n",
      "Sparsity: 29.99%\n",
      "Evaluating CIFAR_Pruned_30%...\n",
      "  Accuracy: 78.41%\n",
      "  Size: 9.44 MB\n",
      "  Latency: 0.8395 ms/image\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing 50% Pruning\n",
      "============================================================\n",
      "\n",
      "[1/2] Pruning MNIST model (50%)...\n",
      "Sparsity: 49.97%\n",
      "Evaluating MNIST_Pruned_50%...\n",
      "  Accuracy: 99.34%\n",
      "  Size: 1.61 MB\n",
      "  Latency: 0.2276 ms/image\n",
      "\n",
      "\n",
      "[2/2] Pruning CIFAR model (50%)...\n",
      "Sparsity: 49.98%\n",
      "Evaluating CIFAR_Pruned_50%...\n",
      "  Accuracy: 78.13%\n",
      "  Size: 9.44 MB\n",
      "  Latency: 0.8348 ms/image\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing 70% Pruning\n",
      "============================================================\n",
      "\n",
      "[1/2] Pruning MNIST model (70%)...\n",
      "Sparsity: 69.96%\n",
      "Evaluating MNIST_Pruned_70%...\n",
      "  Accuracy: 99.34%\n",
      "  Size: 1.61 MB\n",
      "  Latency: 0.1621 ms/image\n",
      "\n",
      "\n",
      "[2/2] Pruning CIFAR model (70%)...\n",
      "Sparsity: 69.97%\n",
      "Evaluating CIFAR_Pruned_70%...\n",
      "  Accuracy: 78.03%\n",
      "  Size: 9.44 MB\n",
      "  Latency: 0.8364 ms/image\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing 90% Pruning\n",
      "============================================================\n",
      "\n",
      "[1/2] Pruning MNIST model (90%)...\n",
      "Sparsity: 89.95%\n",
      "Evaluating MNIST_Pruned_90%...\n",
      "  Accuracy: 98.55%\n",
      "  Size: 1.61 MB\n",
      "  Latency: 0.2554 ms/image\n",
      "\n",
      "\n",
      "[2/2] Pruning CIFAR model (90%)...\n",
      "Sparsity: 89.96%\n",
      "Evaluating CIFAR_Pruned_90%...\n",
      "  Accuracy: 58.95%\n",
      "  Size: 9.44 MB\n",
      "  Latency: 0.8729 ms/image\n",
      "\n",
      "\n",
      "âœ“ Pruning results saved to pruning_results.csv\n",
      "\n",
      "============================================================\n",
      "PRUNING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"PRUNING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "pruning_amounts = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "for prune_amount in pruning_amounts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing {int(prune_amount*100)}% Pruning\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    print(f\"[1/2] Pruning MNIST model ({int(prune_amount*100)}%)...\")\n",
    "    mnist_pruned = copy.deepcopy(mnist_baseline)\n",
    "    mnist_pruned = apply_unstructured_pruning(mnist_pruned, amount=prune_amount)\n",
    "    \n",
    "    \n",
    "    sparsity = get_sparsity(mnist_pruned)\n",
    "    print(f\"Sparsity: {sparsity:.2f}%\")\n",
    "    \n",
    "    metrics = collect_metrics(mnist_pruned, mnist_test_loader, f\"MNIST_Pruned_{int(prune_amount*100)}%\")\n",
    "    metrics['compression_type'] = 'pruning'\n",
    "    metrics['pruning_amount'] = prune_amount\n",
    "    metrics['sparsity'] = sparsity\n",
    "    all_results.append(metrics)\n",
    "    \n",
    "    print(f\"\\n[2/2] Pruning CIFAR model ({int(prune_amount*100)}%)...\")\n",
    "    cifar_pruned = copy.deepcopy(cifar_baseline)\n",
    "    cifar_pruned = apply_unstructured_pruning(cifar_pruned, amount=prune_amount)\n",
    "    \n",
    "    sparsity = get_sparsity(cifar_pruned)\n",
    "    print(f\"Sparsity: {sparsity:.2f}%\")\n",
    "    \n",
    "    metrics = collect_metrics(cifar_pruned, cifar_test_loader, f\"CIFAR_Pruned_{int(prune_amount*100)}%\")\n",
    "    metrics['compression_type'] = 'pruning'\n",
    "    metrics['pruning_amount'] = prune_amount\n",
    "    metrics['sparsity'] = sparsity\n",
    "    all_results.append(metrics)\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv('pruning_results.csv', index=False)\n",
    "print(\"\\nPruning results saved to pruning_results.csv\")\n",
    "\n",
    "print(\"\\nPRUNING COMPLETE!\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization <a id='quantization'></a>\n",
    "\n",
    "The `quantize_model` function deepâ€‘copies a model to CPU and applies PyTorchâ€™s dynamic postâ€‘training quantization to its linear/conv layers, while `get_quantized_model_size` measures a modelâ€™s serialized size in memory to report its storage footprint (cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_model(model):\n",
    "    \"\"\"\n",
    "    Apply post-training dynamic quantization.\n",
    "    \n",
    "    Args:\n",
    "        model: Neural network to quantize\n",
    "    \n",
    "    Returns:\n",
    "        Quantized model\n",
    "    \"\"\"\n",
    "    model_cpu = copy.deepcopy(model).cpu()\n",
    "    model_cpu.eval()\n",
    "   \n",
    "    quantized_model = torch.quantization.quantize_dynamic(\n",
    "        model_cpu,\n",
    "        {nn.Linear, nn.Conv2d},  \n",
    "        dtype=torch.qint8        \n",
    "    )\n",
    "    \n",
    "    return quantized_model\n",
    "\n",
    "\n",
    "def get_quantized_model_size(model):\n",
    "    \"\"\"\n",
    "    Calculate model size without saving to disk.\n",
    "    Uses in-memory size calculation.\n",
    "    \"\"\"\n",
    "    import io\n",
    "    \n",
    "    buffer = io.BytesIO()\n",
    "    \n",
    "    torch.save(model.state_dict(), buffer)\n",
    "    \n",
    "    size_mb = buffer.tell() / 1024**2\n",
    "    \n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUANTIZATION EXPERIMENTS\n",
      "============================================================\n",
      "\n",
      "[1/2] Quantizing MNIST model...\n",
      "âœ“ MNIST model quantized\n",
      "Evaluating quantized MNIST model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_6832\\2669489221.py:17: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  quantized_model = torch.quantization.quantize_dynamic(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 99.34%\n",
      "  Size: 0.46 MB\n",
      "  Latency: 0.1352 ms/image\n",
      "\n",
      "[2/2] Quantizing CIFAR model...\n",
      "âœ“ CIFAR model quantized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harri\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating quantized CIFAR model...\n",
      "  Accuracy: 78.48%\n",
      "  Size: 3.43 MB\n",
      "  Latency: 0.4926 ms/image\n",
      "\n",
      "âœ“ Quantization results saved\n",
      "\n",
      "============================================================\n",
      "QUANTIZATION COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"QUANTIZATION EXPERIMENTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n[1/2] Quantizing MNIST model...\")\n",
    "mnist_quantized = quantize_model(mnist_baseline)\n",
    "print(\"MNIST model quantized\")\n",
    "\n",
    "mnist_test_cpu = DataLoader(\n",
    "    torchvision.datasets.MNIST(root='./data', train=False, transform=mnist_transform),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Evaluating quantized MNIST model...\")\n",
    "mnist_quantized.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(mnist_test_cpu):\n",
    "        start = time.time()\n",
    "        output = mnist_quantized(data)\n",
    "        end = time.time()\n",
    "        \n",
    "        if i > 0:  \n",
    "            times.append((end - start) * 1000 / data.size(0))\n",
    "        \n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "accuracy = 100. * correct / total\n",
    "size_mb = get_quantized_model_size(mnist_quantized)\n",
    "latency_ms = np.mean(times)\n",
    "\n",
    "metrics = {\n",
    "    'model_name': 'MNIST_Quantized_INT8',\n",
    "    'accuracy': accuracy,\n",
    "    'size_mb': size_mb,\n",
    "    'latency_ms': latency_ms,\n",
    "    'compression_type': 'quantization',\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "all_results.append(metrics)\n",
    "\n",
    "print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"  Size: {size_mb:.2f} MB\")\n",
    "print(f\"  Latency: {latency_ms:.4f} ms/image\")\n",
    "\n",
    "print(\"\\n[2/2] Quantizing CIFAR model...\")\n",
    "cifar_quantized = quantize_model(cifar_baseline)\n",
    "print(\"CIFAR model quantized\")\n",
    "\n",
    "cifar_test_cpu = DataLoader(\n",
    "    torchvision.datasets.CIFAR10(root='./data', train=False, transform=cifar_transform),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Evaluating quantized CIFAR model...\")\n",
    "cifar_quantized.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(cifar_test_cpu):\n",
    "        start = time.time()\n",
    "        output = cifar_quantized(data)\n",
    "        end = time.time()\n",
    "        \n",
    "        if i > 0:\n",
    "            times.append((end - start) * 1000 / data.size(0))\n",
    "        \n",
    "        _, predicted = output.max(1)\n",
    "        total += target.size(0)\n",
    "        correct += predicted.eq(target).sum().item()\n",
    "\n",
    "accuracy = 100. * correct / total\n",
    "size_mb = get_quantized_model_size(cifar_quantized)\n",
    "latency_ms = np.mean(times)\n",
    "\n",
    "metrics = {\n",
    "    'model_name': 'CIFAR_Quantized_INT8',\n",
    "    'accuracy': accuracy,\n",
    "    'size_mb': size_mb,\n",
    "    'latency_ms': latency_ms,\n",
    "    'compression_type': 'quantization',\n",
    "    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "}\n",
    "all_results.append(metrics)\n",
    "\n",
    "print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"  Size: {size_mb:.2f} MB\")\n",
    "print(f\"  Latency: {latency_ms:.4f} ms/image\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv('quantization_results.csv', index=False)\n",
    "print(\"\\nQuantization results saved\")\n",
    "\n",
    "print(\"\\nQUANTIZATION COMPLETE!\")\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Distillation <a id='distillation'></a>\n",
    "\n",
    "This `TinyMNIST` class defines a much smaller convolutional network used as a student model in distillation, with two conv+pool layers, reduced channel counts and a smaller fullyâ€‘connected head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Knowledge distillation functions defined\n"
     ]
    }
   ],
   "source": [
    "class TinyMNIST(nn.Module):\n",
    "    \"\"\"\n",
    "    Smaller student model for MNIST.\n",
    "    Much fewer parameters than SimpleMNIST.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TinyMNIST, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1) \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, 64) \n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 32 * 7 * 7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TinyCIFAR(nn.Module):\n",
    "    \"\"\"\n",
    "    Smaller student model for CIFAR-10.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(TinyCIFAR, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)   \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1) \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1) \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 256) \n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, temperature=3.0, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Compute knowledge distillation loss.\n",
    "    \n",
    "    Args:\n",
    "        student_logits: Raw outputs from student model\n",
    "        teacher_logits: Raw outputs from teacher model\n",
    "        labels: True class labels\n",
    "        temperature: Softening parameter (higher = softer)\n",
    "        alpha: Weight between distillation and classification loss\n",
    "    \n",
    "    Returns:\n",
    "        Combined loss\n",
    "    \"\"\"\n",
    "    soft_student = F.log_softmax(student_logits / temperature, dim=1)\n",
    "    soft_teacher = F.softmax(teacher_logits / temperature, dim=1)\n",
    "    \n",
    "    distill_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (temperature ** 2)\n",
    "    \n",
    "    student_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    total_loss = alpha * distill_loss + (1 - alpha) * student_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train_student(student, teacher, train_loader, epochs=10, temperature=3.0, alpha=0.7):\n",
    "    \"\"\"\n",
    "    Train student model using knowledge distillation.\n",
    "    \n",
    "    Args:\n",
    "        student: Small model to train\n",
    "        teacher: Large pre-trained model\n",
    "        train_loader: Training data\n",
    "        epochs: Number of training epochs\n",
    "        temperature: Distillation temperature\n",
    "        alpha: Distillation weight\n",
    "    \"\"\"\n",
    "    teacher.eval() \n",
    "    student.train()\n",
    "    \n",
    "    optimizer = optim.Adam(student.parameters(), lr=0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(data)\n",
    "            \n",
    "            student_logits = student(data)\n",
    "            \n",
    "            loss = distillation_loss(student_logits, teacher_logits, target, temperature, alpha)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = student_logits.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f'\\n>>> Epoch {epoch+1} complete. Avg Loss: {epoch_loss:.4f}, '\n",
    "              f'Train Accuracy: {100.*correct/total:.2f}%\\n')\n",
    "\n",
    "print(\"Knowledge distillation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Knowledge Distillation\n",
    "\n",
    "That block runs the distillation experiments by creating tiny student models for MNIST and CIFAR, printing their size/compression ratios, training them against the preâ€‘trained teacher networks, collecting metrics, and saving the resulting student weights and combined results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KNOWLEDGE DISTILLATION EXPERIMENTS\n",
      "============================================================\n",
      "\n",
      "[1/2] Training MNIST student model...\n",
      "\n",
      "Student model size: 105,866 parameters\n",
      "Teacher model size: 421,642 parameters\n",
      "Compression ratio: 3.98x\n",
      "\n",
      "Epoch 1/10, Batch 0/469, Loss: 14.3606, Acc: 10.94%\n",
      "Epoch 1/10, Batch 100/469, Loss: 3.3159, Acc: 70.41%\n",
      "Epoch 1/10, Batch 200/469, Loss: 1.7718, Acc: 79.77%\n",
      "Epoch 1/10, Batch 300/469, Loss: 1.2835, Acc: 84.15%\n",
      "Epoch 1/10, Batch 400/469, Loss: 1.0060, Acc: 86.83%\n",
      "\n",
      ">>> Epoch 1 complete. Avg Loss: 2.3494, Train Accuracy: 88.10%\n",
      "\n",
      "Epoch 2/10, Batch 0/469, Loss: 0.6981, Acc: 95.31%\n",
      "Epoch 2/10, Batch 100/469, Loss: 0.3798, Acc: 96.20%\n",
      "Epoch 2/10, Batch 200/469, Loss: 0.5299, Acc: 96.53%\n",
      "Epoch 2/10, Batch 300/469, Loss: 0.5114, Acc: 96.63%\n",
      "Epoch 2/10, Batch 400/469, Loss: 0.4651, Acc: 96.76%\n",
      "\n",
      ">>> Epoch 2 complete. Avg Loss: 0.5889, Train Accuracy: 96.78%\n",
      "\n",
      "Epoch 3/10, Batch 0/469, Loss: 0.5059, Acc: 95.31%\n",
      "Epoch 3/10, Batch 100/469, Loss: 0.4594, Acc: 97.61%\n",
      "Epoch 3/10, Batch 200/469, Loss: 0.3564, Acc: 97.70%\n",
      "Epoch 3/10, Batch 300/469, Loss: 0.5444, Acc: 97.70%\n",
      "Epoch 3/10, Batch 400/469, Loss: 0.6324, Acc: 97.75%\n",
      "\n",
      ">>> Epoch 3 complete. Avg Loss: 0.4262, Train Accuracy: 97.78%\n",
      "\n",
      "Epoch 4/10, Batch 0/469, Loss: 0.3556, Acc: 98.44%\n",
      "Epoch 4/10, Batch 100/469, Loss: 0.4737, Acc: 98.12%\n",
      "Epoch 4/10, Batch 200/469, Loss: 0.5581, Acc: 98.13%\n",
      "Epoch 4/10, Batch 300/469, Loss: 0.2901, Acc: 98.07%\n",
      "Epoch 4/10, Batch 400/469, Loss: 0.3901, Acc: 98.05%\n",
      "\n",
      ">>> Epoch 4 complete. Avg Loss: 0.3582, Train Accuracy: 98.08%\n",
      "\n",
      "Epoch 5/10, Batch 0/469, Loss: 0.6795, Acc: 95.31%\n",
      "Epoch 5/10, Batch 100/469, Loss: 0.3616, Acc: 98.26%\n",
      "Epoch 5/10, Batch 200/469, Loss: 0.3359, Acc: 98.37%\n",
      "Epoch 5/10, Batch 300/469, Loss: 0.2382, Acc: 98.34%\n",
      "Epoch 5/10, Batch 400/469, Loss: 0.3095, Acc: 98.36%\n",
      "\n",
      ">>> Epoch 5 complete. Avg Loss: 0.3160, Train Accuracy: 98.32%\n",
      "\n",
      "Epoch 6/10, Batch 0/469, Loss: 0.2592, Acc: 98.44%\n",
      "Epoch 6/10, Batch 100/469, Loss: 0.4154, Acc: 98.40%\n",
      "Epoch 6/10, Batch 200/469, Loss: 0.2180, Acc: 98.55%\n",
      "Epoch 6/10, Batch 300/469, Loss: 0.3562, Acc: 98.55%\n",
      "Epoch 6/10, Batch 400/469, Loss: 0.1970, Acc: 98.56%\n",
      "\n",
      ">>> Epoch 6 complete. Avg Loss: 0.2813, Train Accuracy: 98.54%\n",
      "\n",
      "Epoch 7/10, Batch 0/469, Loss: 0.2312, Acc: 98.44%\n",
      "Epoch 7/10, Batch 100/469, Loss: 0.1633, Acc: 98.83%\n",
      "Epoch 7/10, Batch 200/469, Loss: 0.2299, Acc: 98.73%\n",
      "Epoch 7/10, Batch 300/469, Loss: 0.2831, Acc: 98.70%\n",
      "Epoch 7/10, Batch 400/469, Loss: 0.2113, Acc: 98.75%\n",
      "\n",
      ">>> Epoch 7 complete. Avg Loss: 0.2579, Train Accuracy: 98.75%\n",
      "\n",
      "Epoch 8/10, Batch 0/469, Loss: 0.3020, Acc: 99.22%\n",
      "Epoch 8/10, Batch 100/469, Loss: 0.2515, Acc: 98.85%\n",
      "Epoch 8/10, Batch 200/469, Loss: 0.2327, Acc: 98.83%\n",
      "Epoch 8/10, Batch 300/469, Loss: 0.2554, Acc: 98.84%\n",
      "Epoch 8/10, Batch 400/469, Loss: 0.1792, Acc: 98.84%\n",
      "\n",
      ">>> Epoch 8 complete. Avg Loss: 0.2374, Train Accuracy: 98.84%\n",
      "\n",
      "Epoch 9/10, Batch 0/469, Loss: 0.2254, Acc: 99.22%\n",
      "Epoch 9/10, Batch 100/469, Loss: 0.2783, Acc: 98.96%\n",
      "Epoch 9/10, Batch 200/469, Loss: 0.1916, Acc: 98.92%\n",
      "Epoch 9/10, Batch 300/469, Loss: 0.2546, Acc: 98.92%\n",
      "Epoch 9/10, Batch 400/469, Loss: 0.2148, Acc: 98.94%\n",
      "\n",
      ">>> Epoch 9 complete. Avg Loss: 0.2217, Train Accuracy: 98.95%\n",
      "\n",
      "Epoch 10/10, Batch 0/469, Loss: 0.2042, Acc: 99.22%\n",
      "Epoch 10/10, Batch 100/469, Loss: 0.2750, Acc: 99.07%\n",
      "Epoch 10/10, Batch 200/469, Loss: 0.1998, Acc: 99.08%\n",
      "Epoch 10/10, Batch 300/469, Loss: 0.1576, Acc: 99.01%\n",
      "Epoch 10/10, Batch 400/469, Loss: 0.1460, Acc: 99.05%\n",
      "\n",
      ">>> Epoch 10 complete. Avg Loss: 0.2090, Train Accuracy: 99.01%\n",
      "\n",
      "Evaluating MNIST_Distilled_Student...\n",
      "  Accuracy: 99.08%\n",
      "  Size: 0.40 MB\n",
      "  Latency: 0.1168 ms/image\n",
      "\n",
      "\n",
      "[2/2] Training CIFAR student model...\n",
      "\n",
      "Student model size: 620,362 parameters\n",
      "Teacher model size: 2,473,610 parameters\n",
      "Compression ratio: 3.99x\n",
      "\n",
      "Epoch 1/10, Batch 0/391, Loss: 9.9999, Acc: 11.72%\n",
      "Epoch 1/10, Batch 100/391, Loss: 5.6229, Acc: 33.36%\n",
      "Epoch 1/10, Batch 200/391, Loss: 4.6073, Acc: 40.03%\n",
      "Epoch 1/10, Batch 300/391, Loss: 4.0947, Acc: 44.41%\n",
      "\n",
      ">>> Epoch 1 complete. Avg Loss: 5.1068, Train Accuracy: 47.42%\n",
      "\n",
      "Epoch 2/10, Batch 0/391, Loss: 3.7773, Acc: 60.94%\n",
      "Epoch 2/10, Batch 100/391, Loss: 3.4581, Acc: 62.02%\n",
      "Epoch 2/10, Batch 200/391, Loss: 2.5800, Acc: 63.30%\n",
      "Epoch 2/10, Batch 300/391, Loss: 2.6305, Acc: 64.35%\n",
      "\n",
      ">>> Epoch 2 complete. Avg Loss: 2.8735, Train Accuracy: 64.98%\n",
      "\n",
      "Epoch 3/10, Batch 0/391, Loss: 2.1786, Acc: 67.97%\n",
      "Epoch 3/10, Batch 100/391, Loss: 1.8328, Acc: 69.75%\n",
      "Epoch 3/10, Batch 200/391, Loss: 1.9354, Acc: 70.74%\n",
      "Epoch 3/10, Batch 300/391, Loss: 2.1309, Acc: 71.03%\n",
      "\n",
      ">>> Epoch 3 complete. Avg Loss: 2.0721, Train Accuracy: 71.53%\n",
      "\n",
      "Epoch 4/10, Batch 0/391, Loss: 1.9443, Acc: 76.56%\n",
      "Epoch 4/10, Batch 100/391, Loss: 1.6453, Acc: 75.47%\n",
      "Epoch 4/10, Batch 200/391, Loss: 1.5310, Acc: 75.38%\n",
      "Epoch 4/10, Batch 300/391, Loss: 1.3979, Acc: 75.35%\n",
      "\n",
      ">>> Epoch 4 complete. Avg Loss: 1.6918, Train Accuracy: 75.32%\n",
      "\n",
      "Epoch 5/10, Batch 0/391, Loss: 1.4158, Acc: 78.12%\n",
      "Epoch 5/10, Batch 100/391, Loss: 1.3642, Acc: 77.82%\n",
      "Epoch 5/10, Batch 200/391, Loss: 1.3575, Acc: 78.00%\n",
      "Epoch 5/10, Batch 300/391, Loss: 1.3340, Acc: 77.99%\n",
      "\n",
      ">>> Epoch 5 complete. Avg Loss: 1.4193, Train Accuracy: 78.08%\n",
      "\n",
      "Epoch 6/10, Batch 0/391, Loss: 1.2473, Acc: 85.16%\n",
      "Epoch 6/10, Batch 100/391, Loss: 1.3856, Acc: 80.04%\n",
      "Epoch 6/10, Batch 200/391, Loss: 1.4700, Acc: 80.27%\n",
      "Epoch 6/10, Batch 300/391, Loss: 1.4520, Acc: 80.28%\n",
      "\n",
      ">>> Epoch 6 complete. Avg Loss: 1.2612, Train Accuracy: 80.39%\n",
      "\n",
      "Epoch 7/10, Batch 0/391, Loss: 1.1374, Acc: 86.72%\n",
      "Epoch 7/10, Batch 100/391, Loss: 1.0336, Acc: 82.27%\n",
      "Epoch 7/10, Batch 200/391, Loss: 1.2081, Acc: 82.06%\n",
      "Epoch 7/10, Batch 300/391, Loss: 0.9425, Acc: 82.07%\n",
      "\n",
      ">>> Epoch 7 complete. Avg Loss: 1.1234, Train Accuracy: 82.06%\n",
      "\n",
      "Epoch 8/10, Batch 0/391, Loss: 0.9347, Acc: 86.72%\n",
      "Epoch 8/10, Batch 100/391, Loss: 0.9028, Acc: 84.23%\n",
      "Epoch 8/10, Batch 200/391, Loss: 1.0834, Acc: 83.84%\n",
      "Epoch 8/10, Batch 300/391, Loss: 0.9309, Acc: 83.88%\n",
      "\n",
      ">>> Epoch 8 complete. Avg Loss: 1.0142, Train Accuracy: 83.65%\n",
      "\n",
      "Epoch 9/10, Batch 0/391, Loss: 0.8665, Acc: 86.72%\n",
      "Epoch 9/10, Batch 100/391, Loss: 0.9224, Acc: 85.74%\n",
      "Epoch 9/10, Batch 200/391, Loss: 0.9502, Acc: 85.43%\n",
      "Epoch 9/10, Batch 300/391, Loss: 1.0070, Acc: 85.19%\n",
      "\n",
      ">>> Epoch 9 complete. Avg Loss: 0.9209, Train Accuracy: 85.18%\n",
      "\n",
      "Epoch 10/10, Batch 0/391, Loss: 0.8537, Acc: 89.84%\n",
      "Epoch 10/10, Batch 100/391, Loss: 0.8345, Acc: 86.39%\n",
      "Epoch 10/10, Batch 200/391, Loss: 0.8259, Acc: 86.61%\n",
      "Epoch 10/10, Batch 300/391, Loss: 0.9156, Acc: 86.46%\n",
      "\n",
      ">>> Epoch 10 complete. Avg Loss: 0.8566, Train Accuracy: 86.29%\n",
      "\n",
      "Evaluating CIFAR_Distilled_Student...\n",
      "  Accuracy: 77.17%\n",
      "  Size: 2.37 MB\n",
      "  Latency: 0.4295 ms/image\n",
      "\n",
      "\n",
      "âœ“ All results saved to all_compression_results.csv\n",
      "\n",
      "============================================================\n",
      "KNOWLEDGE DISTILLATION COMPLETE!\n",
      "============================================================\n",
      "\n",
      "ðŸŽ‰ ALL COMPRESSION EXPERIMENTS FINISHED! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "print(\"KNOWLEDGE DISTILLATION EXPERIMENTS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n[1/2] Training MNIST student model...\\n\")\n",
    "mnist_student = TinyMNIST().to(device)\n",
    "print(f\"Student model size: {sum(p.numel() for p in mnist_student.parameters()):,} parameters\")\n",
    "print(f\"Teacher model size: {sum(p.numel() for p in mnist_baseline.parameters()):,} parameters\")\n",
    "print(f\"Compression ratio: {sum(p.numel() for p in mnist_baseline.parameters()) / sum(p.numel() for p in mnist_student.parameters()):.2f}x\\n\")\n",
    "\n",
    "train_student(mnist_student, mnist_baseline, mnist_train_loader, epochs=10)\n",
    "\n",
    "metrics = collect_metrics(mnist_student, mnist_test_loader, \"MNIST_Distilled_Student\")\n",
    "metrics['compression_type'] = 'distillation'\n",
    "all_results.append(metrics)\n",
    "\n",
    "torch.save(mnist_student.state_dict(), 'mnist_student.pth')\n",
    "\n",
    "print(\"\\n[2/2] Training CIFAR student model...\\n\")\n",
    "cifar_student = TinyCIFAR().to(device)\n",
    "print(f\"Student model size: {sum(p.numel() for p in cifar_student.parameters()):,} parameters\")\n",
    "print(f\"Teacher model size: {sum(p.numel() for p in cifar_baseline.parameters()):,} parameters\")\n",
    "print(f\"Compression ratio: {sum(p.numel() for p in cifar_baseline.parameters()) / sum(p.numel() for p in cifar_student.parameters()):.2f}x\\n\")\n",
    "\n",
    "train_student(cifar_student, cifar_baseline, cifar_train_loader, epochs=10)\n",
    "\n",
    "metrics = collect_metrics(cifar_student, cifar_test_loader, \"CIFAR_Distilled_Student\")\n",
    "metrics['compression_type'] = 'distillation'\n",
    "all_results.append(metrics)\n",
    "\n",
    "torch.save(cifar_student.state_dict(), 'cifar_student.pth')\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv('all_compression_results.csv', index=False)\n",
    "print(\"\\nâœ“ All results saved to all_compression_results.csv\")\n",
    "\n",
    "print(\"\\nKNOWLEDGE DISTILLATION COMPLETE!\")\n",
    "print(\"-\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
