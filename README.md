# Edge Inference Outperforms Cloud: Neural Network Compression on NPU-Enabled Devices

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**A systematic evaluation of neural network compression techniques across cloud GPU, modern laptop NPU, and legacy desktop CPU platforms.**

---

## üìã Overview

This repository contains the complete code, data, and figures for the research paper: **"Edge Inference Outperforms Cloud: Evaluating Neural Network Compression on NPU-Enabled Devices"** published in *The Morganton Scientific* (2026).

### Key Findings

üöÄ **Modern laptop NPUs achieve 3√ó faster inference than cloud GPUs**
- Intel Core Ultra 5: 3.5√ó speedup (MNIST), 3.0√ó speedup (CIFAR-10)
- Even legacy 2017 CPUs matched cloud GPU performance

üî• **Quantization amplifies NPU advantages**
- NPU hardware shows 26% larger quantization speedup vs GPU
- INT8 quantization: 3.5√ó size reduction, maintained accuracy, 1.87√ó speedup on NPU

‚ö° **Compression enables 10√ó speedup on legacy hardware**
- Knowledge-distilled student models: 10.48√ó faster on desktop CPU
- Demonstrates viability of older hardware with appropriate compression

üéØ **Task complexity determines pruning tolerance**
- MNIST: Safe up to 70% pruning
- CIFAR-10: Catastrophic degradation at 90% sparsity (19.2% accuracy drop)

---

## üìä Repository Structure

```
.
‚îú‚îÄ‚îÄ README.md                              # This file
‚îú‚îÄ‚îÄ data/                                  # Experimental results
‚îÇ   ‚îú‚îÄ‚îÄ Compression_ML_Paper_Sheet.csv     # Complete results (46 experiments)
‚îÇ   ‚îú‚îÄ‚îÄ paper_summary_table.csv            # Clean summary table
‚îÇ   ‚îî‚îÄ‚îÄ baseline_results.csv               # Baseline measurements
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                             # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ research_notebook_part1.ipynb      # Baseline training & evaluation
‚îÇ   ‚îú‚îÄ‚îÄ research_notebook_part2.ipynb      # Compression experiments
‚îÇ   ‚îî‚îÄ‚îÄ research_notebook_part3_CUSTOM.ipynb # Results visualization
‚îÇ
‚îú‚îÄ‚îÄ src/                                   # Source code
‚îÇ   ‚îú‚îÄ‚îÄ helper_functions.py                # Reusable model & training functions
‚îÇ   ‚îú‚îÄ‚îÄ create_architecture_diagrams.py    # Generate architecture figures
‚îÇ   ‚îî‚îÄ‚îÄ hardware_test.py                   # Hardware capability testing
‚îÇ
‚îú‚îÄ‚îÄ figures/                               # Publication-quality figures (300 DPI)
‚îÇ   ‚îú‚îÄ‚îÄ figure_architectures.png           # Model architecture diagrams
‚îÇ   ‚îú‚îÄ‚îÄ figure_compression_pipeline.png    # Compression methods overview
‚îÇ   ‚îú‚îÄ‚îÄ figure_teacher_student.png         # Knowledge distillation comparison
‚îÇ   ‚îú‚îÄ‚îÄ figure1_platform_comparison.png    # Baseline platform performance
‚îÇ   ‚îú‚îÄ‚îÄ figure2_pruning_sweet_spot.png     # Pruning vs accuracy analysis
‚îÇ   ‚îú‚îÄ‚îÄ figure3_methods_comparison.png     # Compression methods on NPU
‚îÇ   ‚îú‚îÄ‚îÄ figure4_quantization_amplification.png # Platform-specific quantization
‚îÇ   ‚îî‚îÄ‚îÄ figure5_speedup_heatmap.png        # Complete speedup analysis
‚îÇ
‚îú‚îÄ‚îÄ paper/                                 # Paper and documentation
‚îÇ   ‚îú‚îÄ‚îÄ paper_tables.md                    # Table captions and formatting
‚îÇ   ‚îî‚îÄ‚îÄ [Full paper PDF/DOCX]              # Complete manuscript
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt                       # Python dependencies
‚îî‚îÄ‚îÄ LICENSE                                # MIT License

```

---

## üî¨ Experimental Setup

### Datasets
- **MNIST**: 60,000 training / 10,000 test images (28√ó28 grayscale digits)
- **CIFAR-10**: 50,000 training / 10,000 test images (32√ó32 color objects)

### Hardware Platforms
1. **Cloud GPU**: Google Colab with NVIDIA Tesla T4 (16GB)
2. **Laptop NPU**: HP Omnibook with Intel Core Ultra 5 226V (8GB Arc 130V)
3. **Desktop CPU**: Lenovo ThinkCentre with Intel Core i5-8400 (2017)

### Model Architectures
- **SimpleMNIST**: 2 conv blocks ‚Üí 421,642 parameters ‚Üí 1.61 MB
- **SimpleCIFAR**: 3 conv blocks ‚Üí 2,473,610 parameters ‚Üí 9.44 MB

### Compression Techniques
1. **Magnitude-based Pruning**: L1 unstructured at 30%, 50%, 70%, 90% sparsity
2. **Post-training Quantization**: Dynamic INT8 on Conv2d and Linear layers
3. **Knowledge Distillation**: 50% channel reduction ‚Üí 4√ó parameter compression

**Total Experiments**: 46 configurations across methods, datasets, and platforms

---

## üöÄ Quick Start

### Prerequisites
```bash
Python 3.8+
PyTorch 2.0+
CUDA 11.8+ (for GPU experiments)
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/neural-compression-edge-inference.git
cd neural-compression-edge-inference
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Verify hardware setup**
```bash
python src/hardware_test.py
```

Expected output:
```
‚úì Python 3.8+ detected
‚úì PyTorch 2.x.x installed
‚úì Device: cuda / cpu / xpu
‚úì Hardware test complete
```

---

## üìà Reproducing Results

### Option 1: Run Complete Pipeline (Recommended)

**Step 1: Baseline Training (Day 1-2)**
```bash
jupyter notebook notebooks/research_notebook_part1.ipynb
```
- Trains baseline models on MNIST and CIFAR-10
- Evaluates on all available hardware
- Saves: `baseline_results.csv`, model checkpoints

**Step 2: Compression Experiments (Day 3-4)**
```bash
jupyter notebook notebooks/research_notebook_part2.ipynb
```
- Applies pruning, quantization, and distillation
- Tests on all platforms
- Saves: `all_compression_results.csv`

**Step 3: Visualization & Analysis (Day 5)**
```bash
jupyter notebook notebooks/research_notebook_part3_CUSTOM.ipynb
```
- Generates all publication figures
- Creates summary tables
- Performs statistical analysis

**Expected Runtime**:
- Baseline training: 30-60 min (GPU) or 2-3 hours (CPU)
- Compression experiments: 3-5 hours (GPU) or 8-12 hours (CPU)
- Visualization: 5-10 minutes

### Option 2: Use Pre-Generated Results

All experimental data is available in `data/` directory:
```python
import pandas as pd

# Load complete results
results = pd.read_csv('data/Compression_ML_Paper_Sheet.csv')

# Load summary table
summary = pd.read_csv('data/paper_summary_table.csv')
```

### Option 3: Generate Figures Only

```bash
python src/create_architecture_diagrams.py
jupyter notebook notebooks/research_notebook_part3_CUSTOM.ipynb
```

---

## üìä Key Results

### Platform Performance Comparison

| Platform | MNIST Latency | CIFAR Latency | Speedup vs Cloud |
|----------|---------------|---------------|------------------|
| **Cloud GPU (T4)** | 0.565 ms | 2.733 ms | 1.0√ó (baseline) |
| **Laptop NPU (Core Ultra 5)** | 0.160 ms | 0.923 ms | **3.5√ó / 3.0√ó** |
| **Desktop CPU (i5-8400)** | 0.202 ms | 0.900 ms | 2.8√ó / 3.0√ó |

### Compression Methods (Intel Core Ultra 5)

| Method | CIFAR Accuracy | Size | Latency | Speedup |
|--------|----------------|------|---------|---------|
| Baseline | 78.37% | 9.44 MB | 0.923 ms | 1.0√ó |
| Pruning (70%) | 78.03% | 9.44 MB | 0.836 ms | 1.1√ó |
| **Quantization** | **78.48%** | **3.43 MB** | 0.493 ms | **1.9√ó** |
| Distillation | 77.17% | 2.37 MB | **0.430 ms** | 2.1√ó |

### Maximum Speedups Achieved

- **10.48√ó**: CIFAR-10 student model on i5-8400 CPU
- **7.63√ó**: MNIST student model on i5-8400 CPU
- **6.36√ó**: CIFAR-10 distillation on Core Ultra 5
- **5.55√ó**: CIFAR-10 quantization on Core Ultra 5

---

## üîß Hardware-Specific Instructions

### Running on Intel Core Ultra 5 (NPU)

1. **Update Intel drivers**
   - Download latest Arc GPU drivers from Intel
   - Restart after installation

2. **Install Intel Extension for PyTorch**
```bash
pip install intel-extension-for-pytorch --break-system-packages
```

3. **Verify NPU detection**
```bash
python -c "import torch; print(torch.xpu.is_available())"
```

**Note**: Our experiments used CPU backend with NPU acceleration. If XPU is detected, performance may differ.

### Running on Google Colab (Cloud GPU)

1. **Open notebook in Colab**
   - Upload `.ipynb` files to Google Drive
   - Open with Google Colab

2. **Enable GPU runtime**
   - Runtime ‚Üí Change runtime type ‚Üí GPU (T4)

3. **Install dependencies**
```python
!pip install torch torchvision matplotlib pandas seaborn
```

### Running on CPU-Only Systems

Works on any modern CPU. Training will be slower but results are reproducible:
- MNIST baseline: ~30-45 minutes
- CIFAR-10 baseline: ~1-2 hours
- Compression experiments: ~4-8 hours total

---

## üì¶ Requirements

### Core Dependencies
```
torch>=2.0.0
torchvision>=0.15.0
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
scipy>=1.10.0
jupyter>=1.0.0
```

### Optional (for Intel NPU support)
```
intel-extension-for-pytorch>=2.0.0
```

### Install all at once
```bash
pip install -r requirements.txt
```

---

## üìù Citation

If you use this code or data in your research, please cite:

```bibtex
@article{yourname2026edge,
  title={Edge Inference Outperforms Cloud: Evaluating Neural Network Compression on NPU-Enabled Devices},
  author={[Your Name]},
  journal={The Morganton Scientific},
  year={2026},
  publisher={NCSSM}
}
```

---

## ü§ù Contributing

Contributions are welcome! Areas for extension:

- **Additional hardware platforms**: Apple Silicon, Qualcomm NPU, AMD GPUs
- **Larger models**: ResNet, EfficientNet, Vision Transformers
- **More datasets**: ImageNet, COCO, custom domains
- **Combined compression**: Pruning + quantization, quantization + distillation
- **Power measurements**: Energy efficiency comparisons
- **Batch inference**: Performance with varying batch sizes

Please open an issue or pull request with your improvements!

---

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- **Google Colab** for providing free cloud GPU resources
- **PyTorch team** for the excellent deep learning framework
- **Intel** for Core Ultra 5 NPU hardware and developer tools
- **MNIST and CIFAR-10 dataset creators** for benchmark datasets
- **Open-source ML community** for tools and inspiration

Special thanks to Claude (Anthropic) for assistance with code development and debugging throughout this research project.

---

## üìß Contact

**Author**: [Your Name]  
**Email**: [your.email@example.com]  
**Institution**: North Carolina School of Science and Mathematics  
**ORCID**: [Your ORCID ID]

For questions about the research, please open an issue or contact via email.

---

## üìö Additional Resources

- **Paper PDF**: [Link to published paper]
- **Presentation Slides**: [Link if available]
- **Demo Video**: [Link if available]
- **Blog Post**: [Link if available]

---

## üóìÔ∏è Project Timeline

- **August 2025**: Project conception (procrastinated until...)
- **February 9-11, 2026**: Intensive research sprint (Days 1-2)
  - Learned Python and PyTorch from scratch
  - Set up environments on 3 platforms
  - Trained baseline models
- **February 12-13, 2026**: Compression experiments (Days 3-4)
  - Ran 46 experimental configurations
  - Discovered key findings
- **February 14-15, 2026**: Analysis and writing (Days 5-6)
  - Generated all figures and tables
  - Wrote complete manuscript
- **February 16, 2026**: Submission preparation (Day 7)

**Total active development time**: 7 days

---

## üêõ Known Issues / Limitations

1. **Intel Arc GPU not utilized**: Experiments ran on CPU backend with potential NPU acceleration. Direct XPU usage may show different results.

2. **Pruned model sizes unchanged**: PyTorch standard serialization doesn't implement sparse storage. File sizes remain constant despite reduced operations.

3. **Limited to simple CNNs**: Results may not generalize to larger architectures (ResNet, VGG, Transformers).

4. **No power measurements**: Energy efficiency not measured, only inference latency.

5. **Single-image inference**: Batch processing performance not evaluated.

See paper Discussion section for detailed limitations and future work.

---

## üìä Reproducing Paper Figures

All figures in the paper can be regenerated:

**Architecture diagrams (Figures 2-4):**
```bash
python src/create_architecture_diagrams.py
```

**Results figures (Figures 5-9):**
```bash
jupyter notebook notebooks/research_notebook_part3_CUSTOM.ipynb
# Run all cells
```

Figures are saved at 300 DPI in `figures/` directory, ready for publication.

---

## üéì Educational Use

This repository is designed to be educational. Key learning resources:

- **`notebooks/`**: Step-by-step experiments with extensive comments
- **`src/helper_functions.py`**: Reusable code patterns for ML research
- **`paper/`**: Example of complete research paper structure
- **Figures**: Examples of publication-quality visualizations

Ideal for students learning:
- Deep learning fundamentals
- Model compression techniques
- Cross-platform performance evaluation
- Research paper writing
- Scientific Python programming

---

## üîÑ Version History

- **v1.0.0** (February 2026): Initial release with complete research pipeline
  - 3 platforms tested
  - 3 compression methods
  - 46 experiments
  - 8 figures, 3 tables
  - Complete reproducible code

---

## ‚≠ê Star History

If you find this work useful, please consider starring the repository!

---

**Last Updated**: February 16, 2026  
**Repository Status**: ‚úÖ Complete and reproducible
